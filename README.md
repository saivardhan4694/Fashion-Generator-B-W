# Fashion-Generator-B-W
This repository contains GAN network to generate black and white fashion images

# Introduction

The FashionGAN project is a creative exploration into generative adversarial networks (GANs) specifically tailored for generating fashion images. GANs, a powerful subset of machine learning models, consist of two neural networks—the generator and the discriminator—that are trained together to produce realistic images from random noise. The generator learns to create images that mimic real fashion designs, while the discriminator tries to distinguish between the generator's fake images and actual fashion images. Over time, this adversarial process results in the generator producing increasingly convincing fashion images that are almost indistinguishable from real-life examples.

This project utilizes a comprehensive dataset of fashion images to train the GAN model. By feeding the generator network with random noise vectors, it learns to generate a diverse range of fashion items, from casual wear to formal attire, capturing various styles, patterns, and colors. This enables us to explore and visualize new fashion designs that do not exist in the current market, providing insights into potential future trends. The trained generator model can be used to inspire designers or even to automate the initial design process, pushing the boundaries of creativity in fashion.

Through this README, you will find a detailed guide on how to replicate the results, the underlying theory behind GANs, and instructions on how to use the trained model for your own fashion image generation needs.

# Dataset Overview

The dataset used for this project is the FashionMNIST dataset from TensorFlow. FashionMNIST is a popular dataset consisting of grayscale images of fashion items. It contains 60,000 training images and 10,000 testing images, each of size 28x28 pixels, depicting various clothing items such as shirts, trousers, shoes, and bags. The images are labeled into one of ten classes, representing different types of fashion items. This dataset is strictly in black and white, providing a challenging task for generating realistic images through a Generative Adversarial Network (GAN). The simplicity of the images, coupled with the nuanced differences between classes, makes FashionMNIST a compelling choice for exploring generative models in computer vision.

To prepare the dataset for training, standerd tensorflow dataset pipeline was set. First, the images were loaded and normalized using a custom function to scale the pixel values to a range between 0 and 1, which is essential for stable GAN training. The dataset was then cached to improve data retrieval speed, shuffled to ensure randomness in the training process, and batched into groups of 128 images to efficiently manage memory usage and optimize training performance. Finally, prefetching was used to overlap data preprocessing with model execution, reducing the likelihood of bottlenecks and improving the overall efficiency of the training process.

# Model Architecture

The Generator is designed to create realistic images from random noise, essentially learning to mimic the distribution of the training data. The architecture of the Generator is primarily composed of several layers of Transposed Convolutions (also known as Deconvolutions or Up-sampling layers).

## Generator 

The Generator is designed to create realistic images from random noise, essentially learning to mimic the distribution of the training data. The architecture of the Generator is primarily composed of several layers of Transposed Convolutions (also known as Deconvolutions or Up-sampling layers).

Input Layer: The input to the Generator is a random noise vector, typically sampled from a Gaussian distribution. For this project, a 128-dimensional vector is used as input.

Dense Layer: The noise vector is first passed through a dense layer with a significant number of units (e.g., 7x7x256) and reshaped to form a small, low-resolution image-like representation.

Transposed Convolutional Layers: Following the dense layer, a series of transposed convolutional layers are applied. These layers upsample the input, progressively increasing the spatial dimensions of the data (e.g., from 7x7 to 14x14, and finally to 28x28). Each layer is typically followed by a Batch Normalization layer to stabilize training and a ReLU activation function to introduce non-linearity.

Output Layer: The final layer of the Generator uses a transposed convolutional layer with a sigmoid activation function to output an image of size 28x28x1 for grayscale images (or 28x28x3 for color images). The sigmoid activation ensures that the pixel values are in the range [0, 1], matching the normalized data.

## Discriminator

The Discriminator’s role is to distinguish between real images from the training set and fake images generated by the Generator. It is essentially a binary classifier that outputs the probability of an image being real.

Input Layer: The Discriminator takes an image of size 28x28x1 (or 28x28x3 for color images) as input.

Convolutional Layers: The input image is passed through several convolutional layers that progressively reduce the spatial dimensions while increasing the depth of the feature maps. These layers use Leaky ReLU activations, which help to avoid the "dying ReLU" problem and allow the gradient to flow through the network more effectively.

Flatten and Dense Layers: The feature maps are flattened into a one-dimensional vector, which is then passed through one or more dense layers. These dense layers further process the features extracted from the input images.

Output Layer: The final output layer is a dense layer with a single unit and a sigmoid activation function, providing a probability score indicating whether the input image is real (close to 1) or fake (close to 0).

# Training of GAN











